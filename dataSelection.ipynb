{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the first, preliminary selection of relevant review data, we will consider the number of reviews per product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import statistics\n",
    "import spacy\n",
    "import numpy as np\n",
    "from spacy.util import minibatch, compounding\n",
    "#from spacy.tokens import Doc\n",
    "#from spacy.training import Example\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "df = pandas.read_csv(\"Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\", usecols=['id', 'name', 'reviews.doRecommend', 'reviews.rating', 'reviews.text'])\n",
    "productNames = df['name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerProduct = {}\n",
    "for name in productNames:\n",
    "    reviewsPerProduct[name] = len(df[df['name'] == name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection\n",
    "Since some of the products have far more reviews than others, we will for now look at products with more than 20 reviews to avoid giving too much weight to individual reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dict(reviewsPerProduct).items():\n",
    "    if value < 20:\n",
    "        del reviewsPerProduct[key]\n",
    "relevantProducts = list(reviewsPerProduct.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['name'].isin(relevantProducts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of selected reviews is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28174\n"
     ]
    }
   ],
   "source": [
    "print(len(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of selected products is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(relevantProducts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower and upper bounds as well as the average of the number of reviews per product are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "8343\n",
      "853.7575757575758\n"
     ]
    }
   ],
   "source": [
    "print(min(reviewsPerProduct.values()))\n",
    "print(max(reviewsPerProduct.values()))\n",
    "print(statistics.mean(reviewsPerProduct.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average number of characters in the reviews is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.91279193582736\n"
     ]
    }
   ],
   "source": [
    "reviewText = df2['reviews.text'].str.cat(sep='')\n",
    "#words = [ token.lemma_ for token in reviewText if token.is_punct != True ]\n",
    "print(len(reviewText) / len(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percentage of reviews without a 'recommended' attribute (positive or negative) is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.029033861006596\n"
     ]
    }
   ],
   "source": [
    "taggedRevAmount = len(df2[df2['reviews.doRecommend'] == False]) + len(df2[df2['reviews.doRecommend'] == True])\n",
    "print(((len(df2) - taggedRevAmount) / len(df2)) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "The average length of a review in the preliminary selection is 136 characters which is well enough to form an informative sentence. Since reviews with 30 or less characters may be too short to get significant information from, we will filter these reviews out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[df2['reviews.text'].str.len() > 30].copy()\n",
    "df3.columns = ['id', 'name', 'doRecommend', 'rating', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that a large portion of the reviews (43%) have no attribute signifying whether the reviewer recommends the product or not, it would make sense to manually add these attributes using the \"star scheme\" ratings in the reviews. More reliable however would be to assign the 'doRecommend' attributes based on sentiment analysis. The textcat pipeline component will be used for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, the total number of reviews that have passed the earlier filtering steps is 24.239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24239\n"
     ]
    }
   ],
   "source": [
    "print(len(df3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting these into recommending reviews, non recommending reviews, and reviews without such attribute, we get the following numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15318\n",
      "731\n",
      "8190\n"
     ]
    }
   ],
   "source": [
    "print(len(df3[df3['doRecommend'] == True]))\n",
    "print(len(df3[df3['doRecommend'] == False]))\n",
    "print(len(df3[(df3['doRecommend'] != True) & (df3['doRecommend'] != False)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only 731 negative reviews, we will have to limit the training data for our textcat model to 731 positive and 731 negative reviews to balance positive and negative training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_df = df3[df3['doRecommend'] == True][:731]\n",
    "train_neg_df = df3[df3['doRecommend'] == False][:731]\n",
    "train_df = train_pos_df.append(train_neg_df)\n",
    "train_df['tuples'] = train_df.apply(lambda row: (row['text'],int(row['doRecommend'])), axis=1)\n",
    "train = train_df['tuples'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now train our textcat model (The following steps are entirely copied from https://www.kaggle.com/poonaml/text-classification-using-spacy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions from spacy documentation\n",
    "def load_data(limit=0, split=0.8):\n",
    "    train_data = train\n",
    "    np.random.shuffle(train_data)\n",
    "    train_data = train_data[-limit:]\n",
    "    texts, labels = zip(*train_data)\n",
    "    cats = [{'POSITIVE': bool(y)} for y in labels]\n",
    "    split = int(len(train_data) * split)\n",
    "    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n",
    "\n",
    "def evaluate(tokenizer, textcat, texts, cats):\n",
    "    docs = (tokenizer(text) for text in texts)\n",
    "    tp = 1e-8  # True positives\n",
    "    fp = 1e-8  # False positives\n",
    "    fn = 1e-8  # False negatives\n",
    "    tn = 1e-8  # True negatives\n",
    "    for i, doc in enumerate(textcat.pipe(docs)):\n",
    "        gold = cats[i]\n",
    "        for label, score in doc.cats.items():\n",
    "            if label not in gold:\n",
    "                continue\n",
    "            if score >= 0.5 and gold[label] >= 0.5:\n",
    "                tp += 1.\n",
    "            elif score >= 0.5 and gold[label] < 0.5:\n",
    "                fp += 1.\n",
    "            elif score < 0.5 and gold[label] < 0.5:\n",
    "                tn += 1\n",
    "            elif score < 0.5 and gold[label] >= 0.5:\n",
    "                fn += 1\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n",
    "\n",
    "#(\"Number of texts to train from\",\"t\" , int)\n",
    "n_texts=30000\n",
    "#You can increase texts count if you have more computational power.\n",
    "\n",
    "#(\"Number of training iterations\", \"n\", int))\n",
    "n_iter=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading food reviews data...\n",
      "Using 30000 examples (1169 training, 293 evaluation)\n"
     ]
    }
   ],
   "source": [
    "# add the text classifier to the pipeline if it doesn't exist\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'textcat' not in nlp.pipe_names:\n",
    "    textcat = nlp.create_pipe('textcat')\n",
    "    nlp.add_pipe(textcat, last=True)\n",
    "# otherwise, get it, so we can add labels to it\n",
    "else:\n",
    "    textcat = nlp.get_pipe('textcat')\n",
    "\n",
    "# add label to text classifier\n",
    "textcat.add_label('POSITIVE')\n",
    "\n",
    "# load the dataset\n",
    "print(\"Loading food reviews data...\")\n",
    "(train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\n",
    "print(\"Using {} examples ({} training, {} evaluation)\"\n",
    "      .format(n_texts, len(train_texts), len(dev_texts)))\n",
    "train_data = list(zip(train_texts,\n",
    "                      [{'cats': cats} for cats in train_cats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "LOSS \t  P  \t  R  \t  F  \n",
      "4.017\t0.841\t0.731\t0.782\n",
      "1.895\t0.883\t0.883\t0.883\n",
      "1.540\t0.906\t0.862\t0.883\n",
      "0.948\t0.924\t0.841\t0.881\n",
      "1.007\t0.919\t0.855\t0.886\n",
      "0.677\t0.919\t0.855\t0.886\n",
      "0.622\t0.919\t0.855\t0.886\n",
      "0.510\t0.912\t0.862\t0.887\n",
      "0.400\t0.918\t0.848\t0.882\n",
      "0.354\t0.897\t0.897\t0.897\n"
     ]
    }
   ],
   "source": [
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train textcat\n",
    "    optimizer = nlp.begin_training()\n",
    "    print(\"Training the model...\")\n",
    "    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n",
    "    for i in range(n_iter):\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n",
    "                       losses=losses)\n",
    "        with textcat.model.use_params(optimizer.averages):\n",
    "            # evaluate on the dev data split off in load_data()\n",
    "            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n",
    "        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n",
    "              .format(losses['textcat'], scores['textcat_p'],\n",
    "                      scores['textcat_r'], scores['textcat_f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This tea is fun to watch as the flower expands in the water. Very smooth taste and can be used again and again in the same day. If you love tea, you gotta try these \"flowering teas\"',\n",
       " {'POSITIVE': 0.9809731841087341})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the trained model\n",
    "test_text1 = 'This tea is fun to watch as the flower expands in the water. Very smooth taste and can be used again and again in the same day. If you love tea, you gotta try these \"flowering teas\"'\n",
    "test_text2 = \"I bought this product at a local store, not from this seller. I usually use Wellness canned food, but thought my cat was bored and wanted something new. So I picked this up, knowing that Evo is a really good brand (like Wellness). It is one of the most disgusting smelling cat foods I've ever had the displeasure of using. I was gagging while trying to put it into the bowl.  My cat took one taste and walked away, and chose to eat nothing until I replaced it 12 hours later with some dry food. I would try another flavor of their food - since I know it's high quality - but I wouldn't buy the duck flavor again.\"\n",
    "doc = nlp(test_text1)\n",
    "test_text1, doc.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"I bought this product at a local store, not from this seller. I usually use Wellness canned food, but thought my cat was bored and wanted something new. So I picked this up, knowing that Evo is a really good brand (like Wellness). It is one of the most disgusting smelling cat foods I've ever had the displeasure of using. I was gagging while trying to put it into the bowl.  My cat took one taste and walked away, and chose to eat nothing until I replaced it 12 hours later with some dry food. I would try another flavor of their food - since I know it's high quality - but I wouldn't buy the duck flavor again.\",\n",
       " {'POSITIVE': 0.3162754476070404})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(test_text2)\n",
    "test_text2, doc2.cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it should be noted that the steps above are copied - However, the last two, which correspond to testing the textcat model, show results that look very different from where this code is originally from: While one would expect the \"POSITIVE\" score for text 2 to be near 0, it is actually barely below 0.32 which means that the model trained with our data has a stronger bias for categorizing reviews as positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to redefine the reviews that are missing a \"recommended\" attribute using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        I order 3 of them and one of the item is bad q...\n",
       "1        Bulk is always the less expensive way to go fo...\n",
       "2        Well they are not Duracell but for the price i...\n",
       "3        Seem to work as well as name brand batteries a...\n",
       "4        These batteries are very long lasting the pric...\n",
       "                               ...                        \n",
       "12620    If your looking for great sound it cannot perf...\n",
       "12647    I already had a tap upstairs but wanted anothe...\n",
       "12701    Like DOS, you need to memorize your command an...\n",
       "12764    It was just a few weeks ago that I was bemoani...\n",
       "12788    I bought one on Prime day for about 50 shipped...\n",
       "Name: text, Length: 8190, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[(df3['doRecommend'] != True) & (df3['doRecommend'] != False)]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "testerlist = list(nlp.pipe(df3[(df3['doRecommend'] != True) & (df3['doRecommend'] != False)]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POSITIVE': 0.007800657767802477}\n",
      "{'POSITIVE': 0.9470281600952148}\n",
      "{'POSITIVE': 0.5396779179573059}\n"
     ]
    }
   ],
   "source": [
    "print(testerlist[0].cats)\n",
    "print(testerlist[1].cats)\n",
    "print(testerlist[2].cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_examples:\n",
    "#    texts, labels = zip(*train_data)\n",
    "#    for predicted, reference in zip(*train_data):\n",
    "#        predicted = Doc(nlp.vocab, words=predicted)\n",
    "#        examplesList.append(Example(predicted, reference))\n",
    "#    return examplesList"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
